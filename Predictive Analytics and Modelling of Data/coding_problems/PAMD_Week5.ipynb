{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f7da5ff",
   "metadata": {},
   "source": [
    "# Welcome to Week 5 - Data splitting & cross-validation\n",
    "\n",
    "This week we will talk about how to split datasets into training and test data for model building and testing. We will also implement some cross-validation techniques.\n",
    "\n",
    "Before we get started, a little background on random number generators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b212a9",
   "metadata": {},
   "source": [
    "### Section 1: Random number generation\n",
    "\n",
    "Random number(s) can be generated in many ways: \n",
    "\n",
    "1. \"random()\" for a single float number in [0,1),\n",
    "\n",
    "2. \"random.randint(lowerBound, upperBound)\" for an integer within an interval, \n",
    "\n",
    "3. \"random.randrange(lowerBound, upperBound, step)\" for further constraining on the range. (e.g. randrange(0,11,2) randomly generates even number within [0,11) )\n",
    "\n",
    "4. \"random.uniform(a,b)\" for a random float number within interval [a,b). Here a doesn't have to be smaller than b., while method 1-3 will cause TypeError or ValueError if a<b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d91bee",
   "metadata": {},
   "source": [
    "**DEMO 1.1**\n",
    "\n",
    "Here we introduce the random generator from NumPy as follows, which generates an array of length 3 with numbers between 0 and 5 (not including 5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53193a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generates a random sample from a given 1-D array.\n",
    "np.random.choice(5,3)\n",
    "# #This is equivalent to np.random.randint(0,5,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec415ccc",
   "metadata": {},
   "source": [
    "This generation is random. Another run will give you a different array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee1c52de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 1, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(5,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d044bb2",
   "metadata": {},
   "source": [
    "But what if we're testing a specific method and we want to make sure that we \"randomly\" generate the same numbers again?\n",
    "\n",
    "We can achieve that by first setting a \"seed\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f792ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "# We choose seed \"3\" here but you can choose any number. \n",
    "# You can think of it as a passcode - the same seed will produce the same results when you rerun this code later.\n",
    "\n",
    "np.random.choice(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f74b0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "np.random.choice(5,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cdb3f5",
   "metadata": {},
   "source": [
    "Notice how these two arrays are now the same - they are associated with the seed \"3\". Even if you close the notebook and re-run this code later, they will stay the same. But the arrays in the beginning without the seed will change every time. This is because you use a certain stream of previously generated random numbers that are kept in an array, saved via this \"passcode\" seed. In other words, the random samples generated by seed is traceable.\n",
    "\n",
    "Considering we still consider these as random numbers, this is quite counterintuitive. We want to be **random**, but we want **predictable random**. The latter is especially useful if we want to test a piece of code. If we want to find our mistakes or just test the rationale, we don't want to keep running it in order to see whether that one particular instance that was causing an error is reoccurring.\n",
    "\n",
    "In general, to make our sampling appear to be random, as we theorise it should be, we use a computer to generate a random number, or an array of random numbers, depending on the application. Computers do this by making use of **pseudo-random number generators (PRNGs)**. These generators start from a particular state, the (random) seed state, and start to apply different functions/algorithms over that seed to obtain the next results in the sequence. Many such functions exist. The one used in numpy for example is based on the very popular Mersenne Twister PRNG (it is also used in Excel, R, MATLAB, and so on).\n",
    "\n",
    "Now you see why, when we set the seed, we obtain the same results. We are applying the same function to the same starting point so all the subsequent results can be derived by applying the same function. If we don't set the seed in Python, the initial seed value gets reseeded after every generation, i.e., it is replaced by another value from a pseudo-random sequence every time we run the code that is asked to make a selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63efe52",
   "metadata": {},
   "source": [
    "### Section 2: Training and test data / Data splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0297aae3",
   "metadata": {},
   "source": [
    "Supervised learning relies on the training of a model on a set of labelled training data, and then testing the performance of the model on unlabelled test data. In order to do that, we generally split our data randomly into two parts: training and test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9329fb",
   "metadata": {},
   "source": [
    "Remember our linear regression from week 3? Let's try to recreate that model but this time, we will train the model on a part of the data and then test it on the other part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6a5866b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 20640\n",
      "\n",
      "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      "    :Attribute Information:\n",
      "        - MedInc        median income in block group\n",
      "        - HouseAge      median house age in block group\n",
      "        - AveRooms      average number of rooms per household\n",
      "        - AveBedrms     average number of bedrooms per household\n",
      "        - Population    block group population\n",
      "        - AveOccup      average number of household members\n",
      "        - Latitude      block group latitude\n",
      "        - Longitude     block group longitude\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
      "\n",
      "The target variable is the median house value for California districts,\n",
      "expressed in hundreds of thousands of dollars ($100,000).\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "A household is a group of people residing within a home. Since the average\n",
      "number of rooms and bedrooms in this dataset are provided per household, these\n",
      "columns may take surprisingly large values for block groups with few households\n",
      "and many empty houses, such as vacation resorts.\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "      Statistics and Probability Letters, 33 (1997) 291-297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets as ds\n",
    "\n",
    "df = ds.fetch_california_housing(as_frame=True)\n",
    "\n",
    "print(df.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "067e8e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You first have to split your data into dependent and independent part\n",
    "\n",
    "X = df['data']\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f899f",
   "metadata": {},
   "source": [
    "We will now split the data using the train_test_split function from sklearn. Check the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) and try implementing it below. \n",
    "\n",
    "Note the random_state parameter - this refers to the random seed which you can set so that your results become reproducible, just like we did in section 1.\n",
    "\n",
    "The other important parameter is test_size or training size. With either of those you can decide the size of the split.\n",
    "\n",
    "Common split options are 50/50, or 70 training/30 test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a2d59ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecf5268",
   "metadata": {},
   "source": [
    "Now that we've split our data, we will train a linear regression on the TRAIN part of it.\n",
    "\n",
    "Implement this below using the statsmodels OLS function, just like we did in week 3. Make sure to train on JUST the training part of the data this time. You can print a model summary if you like, using the summary() function which statsmodels allows us to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "292e5f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ca056d",
   "metadata": {},
   "source": [
    "Now we want to do out of sample predictions, that is, we want to predict y_test using X_test given the fitted model above.\n",
    "\n",
    "You can do that by running the predict() function for your fitted model, giving it X_test as the parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bbea263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fc7cc8",
   "metadata": {},
   "source": [
    "Time to check our model performance!\n",
    "\n",
    "When evaluating a model, we can look at two types of performance. In the previous lectures, we looked at the in-sample performance (on the training data). That means we measured how well the model was able to predict y given X, and how far away the predictions were. \n",
    "\n",
    "Now, we can also measure the out-of-sample performance (on the test data). This measures how well the fitted model handles unseen data.\n",
    "\n",
    "A good model will do both, but especially the out of sample performance is very important as it tells us how generalisable the model is to new data. A poor out-of-sample performance is indicative of an overfit model.\n",
    "\n",
    "Sklearn has some very useful functions for measuring the performance of your model under sklearn.metrics. You can find a list of them [here](https://scikit-learn.org/stable/modules/model_evaluation.html). \n",
    "\n",
    "The ones for regression include some that you will remember from the lecture:\n",
    "\n",
    "- RMSE / MSE: mean_squared_error (a parameter let's you turn on/off the rooting)\n",
    "- MAE: mean_absolute_error\n",
    "\n",
    "Calculate them below for your model. You will want to look at your predicted values of y compared to your y_test, as that's our out-of-sample performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98bb40fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda211c9",
   "metadata": {},
   "source": [
    "Curious about your in-sample performance? Then you have to rerun the predict() function from earlier on X_train, to give us predictions for those values of X. If you have some extra time in the session today, why now make a comparison of in-sample and out-of-sample performance of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e613e2",
   "metadata": {},
   "source": [
    "By default, the sklearn training/test splitting function tries to **keep the same proportion of each class** that is presented in the original dataset. This will become important in the next computer lab when we talk about over/under sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77745de",
   "metadata": {},
   "source": [
    "### Section 3: Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8043a3",
   "metadata": {},
   "source": [
    "We will now have another look at the training/test splitting function, but this time we will look at a classification case. And in this context we will also talk about k-fold cross validation.\n",
    "\n",
    "Let's generate some random data this time to show you some more neat functions of sklearn.\n",
    "\n",
    "Besides offering us with some real datasets which we've already used, such as the housing data above, sklearn also gives us the option to generate **random aritifical data** specific for model building and testing. \n",
    "\n",
    "You can read more about this functionality [here](https://scikit-learn.org/stable/datasets/sample_generators.html).\n",
    "\n",
    "This time we will use the make_classification function to generate some random data for a logistic regression problem. Check the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html#sklearn.datasets.make_classification) to read more about the parameters with which you can modify the dataset to suit your test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dad54722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X,y = make_classification(n_samples=1000, n_features=10,\n",
    "                               n_informative=2, n_redundant=0, n_repeated=0,\n",
    "                               n_classes=2,\n",
    "                               n_clusters_per_class=1,\n",
    "                               weights=(0.7,0.3),\n",
    "                               class_sep=0.99, random_state=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f259f91",
   "metadata": {},
   "source": [
    "Let's now also create a simple logistic regression model for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "131fd780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd1bd66",
   "metadata": {},
   "source": [
    "We will now make use of the sklearn cross-validation function. There's some great documentation about how sklearn does its cross-validation [here](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation) which I recommend you have a look at.\n",
    "\n",
    "The bottom line is this:\n",
    "- the function holds back a part of the data for final testing\n",
    "- the remaining training data is then split into k parts\n",
    "- in each round, the model is trained on k-1 parts of the data, with 1 being used for interim evaluating of the model\n",
    "- the process is repeated until each part of the training data has been used for this evaluation\n",
    "- the fitted model is used for a final run on the held-back test data\n",
    "- the mean of the interim evaluations is reported\n",
    "\n",
    "**TASK**\n",
    "\n",
    "Implement below the cross_val_score function from sklearn.model_selection on the classifier model defined above. You can choose how many iterations (k) you want to run, but higher numbers will be computationally more expensive so I suggest a value under 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bdd0194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75303610",
   "metadata": {},
   "source": [
    "If everything goes well, the model should report as many accuracy scores as you have folds, i.e., k scores. The default is 'accuracy' but different metrics can be specified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15eb978",
   "metadata": {},
   "source": [
    "If **multiple metrics** are required, apply the function cross_validate(). It basically does the same as the cross_val_score() function, but gives back a whole dict of values.\n",
    "\n",
    "You can read more about it in the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate).\n",
    "\n",
    "**TASK**\n",
    "\n",
    "Implement the function below for the same model above. Note that the parameter return_train_score is set to False as default. If you want to report in-sample (training) evaluation parameters you can set it to true. Generally, we are not very interested in those but there might be situations in which you want to check how they compare to the test performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fa7de65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26955268",
   "metadata": {},
   "source": [
    "### Additional reading / DEMO: Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e786f202",
   "metadata": {},
   "source": [
    "It is generally recommended that any preprocessing and data transformations are first used on the training data, which is then used to build the model.\n",
    "\n",
    "Afterwards, the same pre-processing steps should be applied to the test data separately. This ensures that there is no spillover from any of the transformations between the different data parts, which would mean that there might be information from the test data spilling into the model training.\n",
    "\n",
    "An easy way to implement this is through pipelines.\n",
    "\n",
    "We apply function \"make_pipeline\" from sklearn, to set pipeline of transforms with a final estimator. See documentation [here](https://scikit-learn.org/stable/modules/compose.html#combining-estimators).\n",
    "\n",
    "This function sequentially applies a list of transforms and a final estimator. Intermediate steps of the pipeline must be 'transforms', that is, they must implement fit and transform methods. The final estimator only needs to implement fit. The transformers in the pipeline can be cached using memory argument.\n",
    "\n",
    "The purpose of the pipeline is to **assemble several steps** that can be cross-validated together **while** setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated by a '__'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c16332b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time value: [0.00297856 0.0023787  0.00231791 0.0022316  0.00223565 0.00220942\n",
      " 0.00221086 0.00219202 0.0022037  0.00226426]\n",
      "score_time value: [0.00071263 0.00067329 0.00063705 0.00062466 0.00062418 0.00061846\n",
      " 0.00061154 0.00060034 0.00061989 0.0005734 ]\n",
      "test_accuracy value: [1.   0.96 1.   0.99 0.98 0.99 1.   1.   0.98 1.  ]\n",
      "train_accuracy value: [0.99222222 0.99555556 0.99111111 0.99222222 0.99222222 0.99222222\n",
      " 0.99111111 0.99222222 0.99333333 0.99111111]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "metrics = ['accuracy']\n",
    "\n",
    "# Construct a Pipeline from the given estimators.\n",
    "# add another step of standardizing.\n",
    "pipeline = make_pipeline(StandardScaler(), classifier)\n",
    "\n",
    "outcomes = cross_validate(pipeline, X, y, scoring=metrics, cv=10, return_train_score=True)\n",
    "for metric in outcomes.keys():\n",
    "    print(metric+\" value: \"+str(outcomes[metric]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
