{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ae8aa41",
   "metadata": {},
   "source": [
    "# Welcome to Week 2 of your PAMD computer labs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed14c3d",
   "metadata": {},
   "source": [
    "Last week we got familiar with the Jupyter notebook and Noteable environment that we'll be working on this year. This week, we will continue our theoretical discussions from the lectures and work on data pre-processing, normalisation and missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b78b85",
   "metadata": {},
   "source": [
    "# Task 1 - Binning, dummy encoding and variable transformations\n",
    "\n",
    "**Task 1.1** Binning numeric variables\n",
    "\n",
    "This is a common problems that you'll encounter working with data, so it's important to remember the steps how to do this. Start by loading in our wine dataset again, which we used last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e3244c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
      "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
      "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
      "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
      "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
      "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
      "\n",
      "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
      "0        3.06                  0.28             2.29             5.64  1.04   \n",
      "1        2.76                  0.26             1.28             4.38  1.05   \n",
      "2        3.24                  0.30             2.81             5.68  1.03   \n",
      "3        3.49                  0.24             2.18             7.80  0.86   \n",
      "4        2.69                  0.39             1.82             4.32  1.04   \n",
      "\n",
      "   od280/od315_of_diluted_wines  proline  \n",
      "0                          3.92   1065.0  \n",
      "1                          3.40   1050.0  \n",
      "2                          3.17   1185.0  \n",
      "3                          3.45   1480.0  \n",
      "4                          2.93    735.0  \n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets as datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset = datasets.load_wine()\n",
    "wine = pd.DataFrame(data=dataset['data'], columns=dataset['feature_names'])\n",
    "\n",
    "print(wine.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15c01ba",
   "metadata": {},
   "source": [
    "**Task:** Bin the variable 'flavanoids' into 5 bins using pandas. Make sure that the bins are of equal size, i.e. there is an equal number of elements in each bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbdcef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441fc374",
   "metadata": {},
   "source": [
    "**Task 1.2** Transforming variables\n",
    "\n",
    "Start by creating an array of random colours. To practice some random element generators, you can try creating an array of length 100 where 50% of elements are 'red', 10% are 'blue', 10% are 'green' and the remaining 20% are 'yellow'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a1283f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97d3a05",
   "metadata": {},
   "source": [
    "We discussed in class that categorical variables are often assigned numeric values through dummy encoding. Try that using the pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97a4e80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b872d6",
   "metadata": {},
   "source": [
    "Note how blue isn't assigned a number, because it's taken as the baseline. This is important in analysis for multicollinearity reasons. The drop_first=True command ensures that this is the case, so check what happens if you choose to make it 'False' instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d0fba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a12d721",
   "metadata": {},
   "source": [
    "If you would like to replace each colour with its own assigned integer instead of using dummies, you can try out the label encoder in scikit learn. Use that to create an array of numbers by replacing each colour in colour_array with an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "681ba6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16d2478",
   "metadata": {},
   "source": [
    "# Task 2 - Normalising and detecting outliers\n",
    "\n",
    "Continuing with our wine dataset from earlier, let's look into how to normalise numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60d58114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
      "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
      "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
      "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
      "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
      "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
      "\n",
      "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
      "0        3.06                  0.28             2.29             5.64  1.04   \n",
      "1        2.76                  0.26             1.28             4.38  1.05   \n",
      "2        3.24                  0.30             2.81             5.68  1.03   \n",
      "3        3.49                  0.24             2.18             7.80  0.86   \n",
      "4        2.69                  0.39             1.82             4.32  1.04   \n",
      "\n",
      "   od280/od315_of_diluted_wines  proline  \n",
      "0                          3.92   1065.0  \n",
      "1                          3.40   1050.0  \n",
      "2                          3.17   1185.0  \n",
      "3                          3.45   1480.0  \n",
      "4                          2.93    735.0  \n"
     ]
    }
   ],
   "source": [
    "print(wine.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04ba9c8",
   "metadata": {},
   "source": [
    "**Task 2.1** Min-max-scaling\n",
    "\n",
    "sklearn offers us plenty of pre-processing tools, such as scaling and normalisation functions. Specifically, we will look at [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) and [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html), which are functions in scikit learns preprocessing package.\n",
    "\n",
    "Have a brief look at their documentations to get familiar with the main differences between the two functions. We will start with min-max-scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5790357",
   "metadata": {},
   "source": [
    "Using the MinMaxScaler and its inbuilt function fit_transform(), transform the wine dataset. You can use the default range of (0,1). Save the result in a separate dataframe so that we can compare them later. Use the describe() function to look at the differences between the original and the transformed dataframe, using a variable of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "981d790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b73f05",
   "metadata": {},
   "source": [
    "Plot a variable of your choice, first for the original wine dataset then for the transformed dataframe, to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ba223f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e79c467",
   "metadata": {},
   "source": [
    "**Task 2.2** Normalisation\n",
    "\n",
    "Repeat the same process as above, but using the StandardScaler() function. Compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a265a2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab325c38",
   "metadata": {},
   "source": [
    "Note how the results are the same for the selected range of [0,1] for the min-max-scaler. You can try different values for min and max and watch how the results change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb992fcf",
   "metadata": {},
   "source": [
    "# Task 3 - Outlier detection\n",
    "\n",
    "Scikit learn also has inbuilt functions for detecting outliers. We will be looking at two functions: [EllipticEnvlope](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.EllipticEnvelope.html) and [LocalOutlierFactor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html).\n",
    "\n",
    "EllipticEnvlope can be used for fitting a multivariate Gaussian distribution. It has a parameter contamination rate, indicating the proportion of outliers that will be returned. \n",
    "\n",
    "For LocalOutlierFactor we can define both the number of neighbours 𝑘 and the contamination rate.\n",
    "\n",
    "Check their documentation for more details and then try them out yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff896c2",
   "metadata": {},
   "source": [
    "**Task 3.1** EllipticEnvlope for outlier detection\n",
    "\n",
    "Check for outliers in the wine dataset using the EllipticEnvlope function. You can try out different values for the contamination rate. Save the output from the function as a new column in your wine dataframe, so that we can compare them visually later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c16b04ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee791a15",
   "metadata": {},
   "source": [
    "**Task 3.2** LocalOutlierFactor for outlier detection\n",
    "\n",
    "Now, using the same contamination rate as for the EllipticEnvelope function above, repeat the process using the LocalOutlierFactor function. Again, save your results in a new column in the wine dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c02333af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4514109a",
   "metadata": {},
   "source": [
    "You can now create a comparison between the two solutions, visually or as a table. If you would like to try something more complex, you can create multiple plots showing a comparison of different neighbour and contamination values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "299b4109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeb6960",
   "metadata": {},
   "source": [
    "# Task 4 - Missing values\n",
    "\n",
    "The last task of today is about handling missing values. Our wine dataset is too good for that, though, so let's use a quick dataframe with some missing values that we can tackle. I have uploaded a synthetic example dataset for bank products to Learn which we will use now. Make sure you have uploaded it to your Noteable workspace to ensure you can work with it.\n",
    "\n",
    "It's really small, so you'll be able to print the whole frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a60e4a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Amount          Loan type  Age Gender\n",
      "0   50000.0           Mortgage   19      F\n",
      "1    1000.0           Car loan   23      M\n",
      "2   27000.0           Car loan   44      M\n",
      "3  655555.0           Mortgage   45      F\n",
      "4  187666.0           Mortgage   65      F\n",
      "5  165777.0           Mortgage   39    NaN\n",
      "6       NaN           Mortgage   36      F\n",
      "7  145000.0                NaN   27      F\n",
      "8  156899.0           Mortgage   48      F\n",
      "9   15000.0  Short-term credit   55      M\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('MV_example.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bd3540",
   "metadata": {},
   "source": [
    "**Task 4.1** Dropping NAs\n",
    "\n",
    "One of the easiest ways to handle missing values, is to simply drop (delete) them. Let's first check how many missing values we actually have. In our example we can simply see them, but in the future you will be working with dataframes too big for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afd23a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a345ec",
   "metadata": {},
   "source": [
    "Now drop rows with missing values using a method of your choice. \n",
    "\n",
    "I will be using pandas for that, but feel free to experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2cd0d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52690a5",
   "metadata": {},
   "source": [
    "You can also drop all columns with missing values instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "414893bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4fe696",
   "metadata": {},
   "source": [
    "**Task 4.2** Imputing missing values\n",
    "\n",
    "Instead of dropping them, we can replace missing values with a different value to keep our dataset intact. A quick method for that is to impute them with the mean or the median of that variable. Try that below for the variable \"Amount\".\n",
    "\n",
    "Tip: To calculate the mean or median of a variable which has missing values, you have to use the nanmean() and nanmedian() functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d32c021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
