{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80c389d0",
   "metadata": {},
   "source": [
    "# PAMD computer lab week 4 - Logistic regression + Lasso regression\n",
    "\n",
    "This week's lab will be focused on how to implement logistic regression models, how to evaluate them using confusion matrices and ROC scores, and how lasso regression can be used to choose the most important variables.\n",
    "\n",
    "Our dataset today will be the breast_cancer datafrom skicit learn.\n",
    "\n",
    "First, have a look at the dataset documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html) to explain what the data will look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "128a91cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry\n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        worst/largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
      "        10 is Radius SE, field 20 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn.datasets as ds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Let's load in the data and organise it as predictors (X) and to-be-predicted target class (Y).\n",
    "dataset_class = ds.load_breast_cancer()\n",
    "X_class = pd.DataFrame(data = dataset_class['data'], columns = dataset_class['feature_names'])\n",
    "y_class = pd.DataFrame(data = dataset_class['target'], columns = ['target'])\n",
    "\n",
    "print(dataset_class.DESCR) #This calls the general description of the data, including number of observations, \n",
    "# source, and some descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5a251426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
      "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
      "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
      "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
      "       'smoothness error', 'compactness error', 'concavity error',\n",
      "       'concave points error', 'symmetry error', 'fractal dimension error',\n",
      "       'worst radius', 'worst texture', 'worst perimeter', 'worst area',\n",
      "       'worst smoothness', 'worst compactness', 'worst concavity',\n",
      "       'worst concave points', 'worst symmetry', 'worst fractal dimension'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(X_class.columns) #This calls the names of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c3202ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   target\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n"
     ]
    }
   ],
   "source": [
    "print(y_class.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84995e8",
   "metadata": {},
   "source": [
    "## Task 1: Implement simple logistic regression\n",
    "\n",
    "Start just like we did last week with a simple regression model with just one X variable. I recommend that you use the scikit learn function LogisticRegression() for that. Have a look at the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) as that will help you get started. I typically first have a look at the parameters that the function takes, and then have a look at the implementation example. I then try to replicate that example with my data.\n",
    "\n",
    "As we first want to implement just a simple logistic regression, pick one independent variable as your predictor. You can choose any one of the available ones.\n",
    "\n",
    "One important parameter will be the **solver**. You will remember that we are looking for the best-fit parameters to fit our model to the data. The solver is the approach that the algorithm uses to achieve that. In this example, I would recommend using the 'liblinear' solver. It works best for smaller datasets, which we have here with just around 570 observations.\n",
    "\n",
    "Then the process is very similar to that of implementing the linear regression last week. Try it out yourself below.\n",
    "\n",
    "- Implement a simple logistic regression using the scikit learn function\n",
    "- Choose any independent variable as your X\n",
    "- use the 'liblinear' solver\n",
    "- save your predictions in a separate array for later evaluation\n",
    "\n",
    "Tip: You will notice that the fit() function wants your Y to be an array, but you currently have it as a dataframe column. You can use the [numpy reshape() function](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html) to change the format of it with y_class.values.reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9548c3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569,)\n",
      "[0 0 0 1 0 1 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1\n",
      " 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 0\n",
      " 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_class = X_class[\"mean radius\"]\n",
    "X_class = X_class.values.reshape(-1, 1)\n",
    "y_class = y_class.values.reshape(-1)\n",
    "print(y_class.shape)\n",
    "\n",
    "logModel = LogisticRegression(solver=\"liblinear\")\n",
    "\n",
    "logModel.fit(X_class, y_class)\n",
    "\n",
    "predictions = logModel.predict(X_class)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1ff6db",
   "metadata": {},
   "source": [
    "### 1.2 Logistic regression evaluation and confusion matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f928b947",
   "metadata": {},
   "source": [
    "We now want to evaluate our results. Plotting isn't easy in this case, as we're only predicting a class. Instead, what we usually do, is **count** the number of correct predictions.\n",
    "\n",
    "I will do that using a for-loop, where I simply check whether the predicted value (which I saved in an array from my model above) is the same as the y_class.value at that i. You can use any other approach which feels better for you. Just try to count the number of correctly classified elements (where prediction==actual)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "029414da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504\n",
      "0.8857644991212654\n"
     ]
    }
   ],
   "source": [
    "correctCount = 0\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == y_class[i]:\n",
    "        correctCount += 1\n",
    "\n",
    "print(correctCount)\n",
    "print(correctCount / len(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc5c72e",
   "metadata": {},
   "source": [
    "But what does that number of correct elements actually tell us? It can be a good enough measure depending on what you want to know.\n",
    "\n",
    "**BUT!**\n",
    "\n",
    "From the lecture you will remember that sometimes we don't only care about which elements we correctly predicted, but the errors we made.\n",
    "\n",
    "There are four cases that you might get:\n",
    "\n",
    "- Label is TRUE and you predict TRUE -> True Positive (TP)\n",
    "- Label is FALSE and you predict FALSE -> True Negative (TN)\n",
    "- Label is TRUE and you predict FALSE -> False Negative (FN)\n",
    "- Label is FALSE and you predict TRUE -> False Positive (FP)\n",
    "\n",
    "You will remember that depending on your application context, false positive and false negatives can be worse. Think about our breastcancer dataset here. What do you think would be the worst outcome?\n",
    "\n",
    "A simple way for extracting the different cases above is using the scikit learn function confusion_matrix(). Try using that function below, you just have to input arrays with your prediction and the real values y_class.values, just like we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ec4ed032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[158  54]\n",
      " [ 11 346]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conMatrix = confusion_matrix(y_class, predictions)\n",
    "\n",
    "print(conMatrix)\n",
    "\n",
    "# ADD YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf53527",
   "metadata": {},
   "source": [
    "If you check the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) you will see that we can also extract the TP,TN,FN and FP values as an array using the ravel() function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf0ee0e",
   "metadata": {},
   "source": [
    "### 1.3 Other evaluation metrics (accuracy, recall, precision, specificity)\n",
    "\n",
    "Some other measures of model evaluation related to TP,TN,FN and FP are \n",
    "\n",
    "- accuracy (TP+TN)/(TP+FP+FN+TN)\n",
    "- recall (TP)/(TP+FN) \n",
    "- precision (TP)/(TP+FP) \n",
    "- specificity (TN)/(TN+FP) \n",
    "\n",
    "They can all be calculated using the values acquired above. I will demonstrate that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7eae3683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.8857644991212654\n",
      "Recall 0.7452830188679245\n",
      "Precision 0.9349112426035503\n",
      "Specificity 0.969187675070028\n"
     ]
    }
   ],
   "source": [
    "# Get values from our confusion_matrix function using ravel()\n",
    "\n",
    "TP, FP, FN, TN = confusion_matrix(predictions, y_class).ravel()\n",
    "\n",
    "\n",
    "def calculate_accuracy(TP, FP, FN, TN):\n",
    "    acc = 0\n",
    "    acc = (TP + TN) / (TP + FP + FN + TN)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def calculate_recall(TP, FP, FN, TN):\n",
    "    recall = 0\n",
    "    recall = (TP) / (TP + FN)\n",
    "    return recall\n",
    "\n",
    "\n",
    "def calculate_specificity(TP, FP, FN, TN):\n",
    "    spec = 0\n",
    "    spec = (TN) / (TN + FP)\n",
    "    return spec\n",
    "\n",
    "\n",
    "def calculate_precision(TP, FP, FN, TN):\n",
    "    prec = 0\n",
    "    prec = (TP) / (TP + FP)\n",
    "    return prec\n",
    "\n",
    "\n",
    "print(\"Accuracy \" + str(calculate_accuracy(TP, FP, FN, TN)))\n",
    "print(\"Recall \" + str(calculate_recall(TP, FP, FN, TN)))\n",
    "print(\"Precision \" + str(calculate_precision(TP, FP, FN, TN)))\n",
    "print(\"Specificity \" + str(calculate_specificity(TP, FP, FN, TN)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3210042e",
   "metadata": {},
   "source": [
    "Other measures you could read about in addition to the above if you're curious are fall-out and the f1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6085d0e",
   "metadata": {},
   "source": [
    "### 1.4 ROC curve\n",
    "\n",
    "An important visualisation and evaluation tool for classification models is the \"receiver operating characteristic curve\" or **ROC curve**.\n",
    "\n",
    "It plots the True Positive and False Positive rates, and we often calculate the area under the curve (AUC) as a measure of model fit.\n",
    "\n",
    "As usual, scikit learn provides us with an easy to use way of implementing both a plot of the ROC and the calculation of the AUC. Using the two functions, for which you can read the documentation [here for ROC](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) and [here for AUC](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html), try plotting a ROC curve for our classification above and calculate the area under the curve AUC.\n",
    "\n",
    "Tip: If you're a bit stuck, have a look at the AUC documentation above in particular. The implementation example is easy to follow and you can use that as your guide for the creation of the ROC element and the calculation of the AUC. Then you just have to plot the TPR (y-axis) and FPR (x-axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c81fcbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve as roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ca2459b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158 11 54 346\n",
      "[[0.82823148 0.17176852]\n",
      " [0.95137969 0.04862031]\n",
      " [0.92386828 0.07613172]\n",
      " ...\n",
      " [0.69391566 0.30608434]\n",
      " [0.95212757 0.04787243]\n",
      " [0.01832669 0.98167331]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x175d2c66190>]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(TP, FP, FN, TN)\n",
    "y_scores = logModel.predict_proba(X_class)\n",
    "print(y_scores)\n",
    "\n",
    "rocValues = roc(y_class, y_scores[:, 1])\n",
    "falsePositiveRate, truePositiveRate = rocValues[0], rocValues[1]\n",
    "plt.plot(falsePositiveRate, truePositiveRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6927f875",
   "metadata": {},
   "source": [
    "## Optional: Additional tasks\n",
    "\n",
    "If you would like to continue this example, try implementing a multiple logistic regression next. Does your model improve its prediction accuracy? Compare the error rates or the AUC values. You can also try plotting the two ROCs from the simple and the multiple model together to visualise the improvement.\n",
    "\n",
    "If you'd rather move on to the next example, have a look at the below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093f4d85",
   "metadata": {},
   "source": [
    "## Demonstration: Logistic regression with Lasso\n",
    "\n",
    "The last thing I wanted to introduce today is how you can use Lasso to improve a logistic regression model by choosing the best combination of predictors. I will demonstrate this using a multi-class logistic regression example, to make it a bit more interesting, but the same logic can be applied to our binary example from above.\n",
    "\n",
    "Specifically, we will come back to our old friend the [Wine data set](https://scikit-learn.org/stable/datasets/toy_dataset.html#wine-dataset):\n",
    "\n",
    "This dataset consists of 178 instances and 13 variables, describing different wines and their characteristics. The dataset also gives us three classes\n",
    "\n",
    "class_0\n",
    "class_1\n",
    "class_2\n",
    "\n",
    "which we want to predict.\n",
    "\n",
    "The class distribution is relatively evenly spread:\n",
    "\n",
    "class_0 (59), class_1 (71), class_2 (48)\n",
    "\n",
    "Remember that in some instances, when you're trying to predict particularly rare classes, you might need to do some pre-processing of the data (e.g. through oversampling). Otherwise, you run into the risk of the predictive model not being able to fit properly to those classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0dc041a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by importing today's packages\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "\n",
    "# Load the dataset and specify which part will be the predictors and which the to-be-predicted outcome\n",
    "dataset = ds.load_wine()\n",
    "X = dataset['data']\n",
    "y = dataset['target']\n",
    "\n",
    "# Split data, here 70/30 split\n",
    "x_train, x_test, y_train, y_test = tts(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2412d4a9",
   "metadata": {},
   "source": [
    "Remember that logistic regression (just like linear regression last week) is sensitive towards scale differences. We therefore scale our predictors. Output variables don't usually need scaling, as we are not making direct comparisons among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "39dcab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled predictors\n",
    "x_train_scaled = StandardScaler().fit_transform(x_train)\n",
    "x_test_scaled = StandardScaler().fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fc52749e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>13.000618</td>\n",
       "      <td>2.336348</td>\n",
       "      <td>2.366517</td>\n",
       "      <td>19.494944</td>\n",
       "      <td>99.741573</td>\n",
       "      <td>2.295112</td>\n",
       "      <td>2.029270</td>\n",
       "      <td>0.361854</td>\n",
       "      <td>1.590899</td>\n",
       "      <td>5.058090</td>\n",
       "      <td>0.957449</td>\n",
       "      <td>2.611685</td>\n",
       "      <td>746.893258</td>\n",
       "      <td>0.938202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.811827</td>\n",
       "      <td>1.117146</td>\n",
       "      <td>0.274344</td>\n",
       "      <td>3.339564</td>\n",
       "      <td>14.282484</td>\n",
       "      <td>0.625851</td>\n",
       "      <td>0.998859</td>\n",
       "      <td>0.124453</td>\n",
       "      <td>0.572359</td>\n",
       "      <td>2.318286</td>\n",
       "      <td>0.228572</td>\n",
       "      <td>0.709990</td>\n",
       "      <td>314.907474</td>\n",
       "      <td>0.775035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>11.030000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>1.280000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>1.270000</td>\n",
       "      <td>278.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12.362500</td>\n",
       "      <td>1.602500</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>1.742500</td>\n",
       "      <td>1.205000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>3.220000</td>\n",
       "      <td>0.782500</td>\n",
       "      <td>1.937500</td>\n",
       "      <td>500.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.050000</td>\n",
       "      <td>1.865000</td>\n",
       "      <td>2.360000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>2.355000</td>\n",
       "      <td>2.135000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>1.555000</td>\n",
       "      <td>4.690000</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>2.780000</td>\n",
       "      <td>673.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13.677500</td>\n",
       "      <td>3.082500</td>\n",
       "      <td>2.557500</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>2.875000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>1.120000</td>\n",
       "      <td>3.170000</td>\n",
       "      <td>985.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14.830000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.230000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>3.880000</td>\n",
       "      <td>5.080000</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>3.580000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.710000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1680.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          alcohol  malic_acid         ash  alcalinity_of_ash   magnesium  \\\n",
       "count  178.000000  178.000000  178.000000         178.000000  178.000000   \n",
       "mean    13.000618    2.336348    2.366517          19.494944   99.741573   \n",
       "std      0.811827    1.117146    0.274344           3.339564   14.282484   \n",
       "min     11.030000    0.740000    1.360000          10.600000   70.000000   \n",
       "25%     12.362500    1.602500    2.210000          17.200000   88.000000   \n",
       "50%     13.050000    1.865000    2.360000          19.500000   98.000000   \n",
       "75%     13.677500    3.082500    2.557500          21.500000  107.000000   \n",
       "max     14.830000    5.800000    3.230000          30.000000  162.000000   \n",
       "\n",
       "       total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n",
       "count     178.000000  178.000000            178.000000       178.000000   \n",
       "mean        2.295112    2.029270              0.361854         1.590899   \n",
       "std         0.625851    0.998859              0.124453         0.572359   \n",
       "min         0.980000    0.340000              0.130000         0.410000   \n",
       "25%         1.742500    1.205000              0.270000         1.250000   \n",
       "50%         2.355000    2.135000              0.340000         1.555000   \n",
       "75%         2.800000    2.875000              0.437500         1.950000   \n",
       "max         3.880000    5.080000              0.660000         3.580000   \n",
       "\n",
       "       color_intensity         hue  od280/od315_of_diluted_wines      proline  \\\n",
       "count       178.000000  178.000000                    178.000000   178.000000   \n",
       "mean          5.058090    0.957449                      2.611685   746.893258   \n",
       "std           2.318286    0.228572                      0.709990   314.907474   \n",
       "min           1.280000    0.480000                      1.270000   278.000000   \n",
       "25%           3.220000    0.782500                      1.937500   500.500000   \n",
       "50%           4.690000    0.965000                      2.780000   673.500000   \n",
       "75%           6.200000    1.120000                      3.170000   985.000000   \n",
       "max          13.000000    1.710000                      4.000000  1680.000000   \n",
       "\n",
       "           target  \n",
       "count  178.000000  \n",
       "mean     0.938202  \n",
       "std      0.775035  \n",
       "min      0.000000  \n",
       "25%      0.000000  \n",
       "50%      1.000000  \n",
       "75%      2.000000  \n",
       "max      2.000000  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit everything together nicely into one dataframe after scaling in the previous step.\n",
    "\n",
    "data1 = pd.DataFrame(data= np.c_[dataset['data'], dataset['target']],\n",
    "                     columns= dataset['feature_names'] + ['target'])                     \n",
    "\n",
    "data1.describe()\n",
    "\n",
    "# Note that the descriptives don't actually tell us anything about our target variable, because it's a label.\n",
    "# You can see, however, that there doesn't seem to be any missing data (all counts 178) and you can also\n",
    "# use these statistics to get an idea of the distribution of your predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c4b524",
   "metadata": {},
   "source": [
    "We now want to fit a logistic regression model to predict the membership in a group.\n",
    "\n",
    "In this case, we choose a penalized regression model. You will remember that these models allow us to reduce the number of predictors (in this case 13 of them) to only contain the most informative ones.\n",
    "\n",
    "For Lasso regression, the choice of the penalty factor is quite important. Thinking back to the lecture slides, this is your tuning parameter lambda. It describes how much you want to penalize for any added parameter.\n",
    "\n",
    "- A lambda of 0 implies that all predictors should be kept and no penalty should be applied.\n",
    "- A lambda of infinity would consider no predictors at all and a maximum penalty. Usually, we restrict the parameter to max out at 1 though.\n",
    "- With an increasing lambda, we consider fewer predictors. This increases the bias of the model. High bias is related to underfitting your model.\n",
    "- With a decreasing lambda, we consider more predictors. This increases the variance of the model. High variance is related to overfitting.\n",
    "\n",
    "In order to choose an optimal lambda for our case, we run k-fold cross validation to tune it while accounting for the model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3912205c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'C': 0.4901}\n"
     ]
    }
   ],
   "source": [
    "from numpy import arange\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lasso_logistic_model = LogisticRegression(\n",
    "    penalty='l1', # L1 penalty refers to Lasso regression (L2 would be ridge regression and 'elasticnet' elastic net)\n",
    "    solver='liblinear') # we choose this solver here because it's the fastest for small datasets\n",
    "\n",
    "\n",
    "# Let's now look for the optimal value of lambda:\n",
    "\n",
    "grid = dict() \n",
    "grid['C'] = arange(0.0001, 1, 0.01)\n",
    "\n",
    "# 5-fold cross validation, evaluating by model accuracy\n",
    "search = GridSearchCV(lasso_logistic_model, grid, scoring='accuracy', cv=5, refit=True)\n",
    "results = search.fit(x_train_scaled, y_train)\n",
    "\n",
    "print('Config: %s' % results.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c7f3b9",
   "metadata": {},
   "source": [
    "We have identified our optimal lambda in this case to be 0.4901.\n",
    "\n",
    "But what is the actual impact of this penalization term on the results of our logistic regression?\n",
    "\n",
    "\n",
    "Let's visualize the coefficients of the lasso-logistic model and compare with those of the standard (non-penalized) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e95c3335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients of the lasso logistic model: \n",
      "\n",
      " [[ 1.03441761  0.          0.56456559 -1.0412312   0.          0.\n",
      "   0.93411253  0.          0.          0.          0.          0.46108538\n",
      "   1.64088557]\n",
      " [-1.41022274 -0.08099093 -0.80037343  0.46466053  0.          0.\n",
      "   0.15680854  0.          0.29090555 -1.29697742  1.05495477  0.\n",
      "  -1.41953695]\n",
      " [ 0.          0.17317128  0.1556954   0.          0.          0.\n",
      "  -1.55778317  0.          0.          1.29754735 -0.79328054 -0.64346563\n",
      "   0.        ]]\n",
      "\n",
      "\n",
      " Coefficients of the traditional logistic model: \n",
      "\n",
      " [[ 4.07894388  1.40496807  2.6084901  -4.70856028  0.65505705  1.16054494\n",
      "   2.13844972  0.65351972  1.11618752  0.3218668  -0.1976088   3.00237699\n",
      "   4.26715433]\n",
      " [-6.59464113 -0.8653589  -5.73187583  4.49811726  0.26276841 -2.24041925\n",
      "   3.08249089  0.82525615  1.7858793  -4.29955537  5.63507782 -0.32811002\n",
      "  -6.42966757]\n",
      " [ 2.51569726 -0.53960917  3.12338573  0.21044302 -0.91782547  1.07987431\n",
      "  -5.22094061 -1.47877587 -2.90206682  3.97768857 -5.43746902 -2.67426698\n",
      "   2.16251324]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\redhawc\\.pyenv\\pyenv-win\\versions\\3.11.2\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Coefficients of the lasso-logistic model\n",
    "lasso_model = LogisticRegression(\n",
    "    penalty='l1', # Lasso regression\n",
    "    solver='liblinear',\n",
    "    C = 0.4901).fit(x_train_scaled,y_train) #here is your new penalty term from before\n",
    "print(\"Coefficients of the lasso logistic model: \\n\\n\",lasso_model.coef_)\n",
    "\n",
    "# Coefficients of the traditional logistic model\n",
    "logistic_model = LogisticRegression( # use the same model\n",
    "    penalty='none').fit(x_train_scaled,y_train) # but without the penalty\n",
    "print(\"\\n\\n Coefficients of the traditional logistic model: \\n\\n\",logistic_model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ffa90c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 0, 1, 0, 1, 2, 1, 2, 0, 2, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 2, 2, 2, 1, 1, 1, 0, 0, 1, 2, 0, 0, 0, 2, 2, 1, 2, 0, 1, 1, 1,\n",
       "       2, 0, 1, 1, 2, 0, 1, 0, 0, 2])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_model.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39660f61",
   "metadata": {},
   "source": [
    "Note how the output from the scikit learn package are not as \"pretty\" as the ones we got last week from our statsmodel summary. Indeed, scitkit learn outputs don't have a summary attribute, which means no p-values to look at!\n",
    "\n",
    "In order to get those, we could use the statsmodel package. \n",
    "\n",
    "But today we're not that interested in p-value comparisons, so let's instead continue comparing predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7dab4c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Predictions of the Lasso logistic model: \n",
      "\n",
      " [0 0 2 0 1 0 1 2 1 2 0 2 0 1 0 1 1 1 0 1 0 1 1 2 2 2 1 1 1 0 0 1 2 0 0 0 2\n",
      " 2 1 2 0 1 1 1 2 0 1 1 2 0 1 0 0 2]\n",
      "\n",
      "\n",
      " Predictions of the traditional logistic model: \n",
      "\n",
      " [0 0 2 0 1 0 1 2 1 2 0 2 0 1 0 1 1 1 0 1 0 1 1 2 2 2 1 1 1 0 0 1 2 0 0 0 2\n",
      " 2 1 2 0 1 1 2 2 0 1 1 2 0 1 0 0 2]\n"
     ]
    }
   ],
   "source": [
    "# Predictions with the lasso-logistic model\n",
    "predictions_tuned_model = search.predict(x_test_scaled) # get predictions for our test data\n",
    "# equivalent to: predictions_tuned_model = lasso_model.predict(x_test_scaled)\n",
    "#\n",
    "print(\"\\n\\n Predictions of the Lasso logistic model: \\n\\n\", predictions_tuned_model)\n",
    "\n",
    "# Predictions with the standard logistic model\n",
    "logictic_model = LogisticRegression(solver='liblinear').fit(x_train_scaled,y_train)\n",
    "\n",
    "predictions_non_tuned_model = logictic_model.predict(x_test_scaled)\n",
    "\n",
    "print(\"\\n\\n Predictions of the traditional logistic model: \\n\\n\", predictions_non_tuned_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52ca2a9",
   "metadata": {},
   "source": [
    "Let's evaluate the performance of each model by looking at their confusion matrices.\n",
    "\n",
    "In problems with a small number of classes, they can give you a good overview of correctly and incorrectly classified elements.\n",
    "\n",
    "We also compute the accuracy of both models based on the number of correctly/incorrectly classified instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f4d5627c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for the lasso-logistic model: \n",
      "[[19  0  0]\n",
      " [ 0 21  0]\n",
      " [ 0  0 14]]\n",
      "Confusion matrix for the traditional logistic model: \n",
      "[[19  0  0]\n",
      " [ 0 20  1]\n",
      " [ 0  0 14]]\n",
      "Accuracy lasso model: 1.0\n",
      "Accuracy traditional model: 0.9814814814814815\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "\n",
    "print(\"Confusion matrix for the lasso-logistic model: \\n\"+str(cm(y_test,predictions_tuned_model)))\n",
    "\n",
    "print(\"Confusion matrix for the traditional logistic model: \\n\"+str(cm(y_test,predictions_non_tuned_model)))\n",
    "\n",
    "print(\"Accuracy lasso model: \"+str(accuracy(y_test,predictions_tuned_model)))\n",
    "\n",
    "print(\"Accuracy traditional model: \"+str(accuracy(y_test,predictions_non_tuned_model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529d14f5",
   "metadata": {},
   "source": [
    "From these outputs, we can see that our tuned lasso logistic model outperforms the traditional one.\n",
    "\n",
    "Discussion question: how significant should an improvement be to \"justify\" the additional work that has gone into the lasso regression model? Maybe there is a trade-off due to the additional computational costs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32249f4",
   "metadata": {},
   "source": [
    "## Optional: Additional tasks for logistic regression with Lasso\n",
    "\n",
    "The above was a more advanced demonstration to give you some inspiration for your own work. You can use this as a guideline to play around with some of the introduced functions. You can for example implement a Lasso regression for our breastcancer dataset. Or maybe you want to change the penalisation parameter we optimized to something else and compare the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
